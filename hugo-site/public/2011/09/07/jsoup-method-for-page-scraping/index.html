<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>JSoup Method for Page Scraping | DavidCraddock.net</title>
<meta name="keywords" content="">
<meta name="description" content="I&rsquo;m currently in the process of writing a web scraper for the forums on Gaia Online. Previously, I used to use Python to develop web scrapers, with the very handy Python library BeautifulSoup. Java has an equivalent called JSoup.
Here I have written a class which is extended by each class in my project that wants to scrape HTML. This &lsquo;Scraper&rsquo; class deals with the fetching of the HTML and converting it into a JSoup tree to be navigated and have the data picked out of.">
<meta name="author" content="">
<link rel="canonical" href="http://davidcraddock.net/2011/09/07/jsoup-method-for-page-scraping/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.7da7716a1f2d0725f74c6ae7f8d6adafc43aabe2b366b65bfbf433448e2a2001.css" integrity="sha256-fadxah8tByX3TGrn&#43;Natr8Q6q&#43;KzZrZb&#43;/QzRI4qIAE=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://davidcraddock.net/favicon.ico">
<link rel="apple-touch-icon" href="http://davidcraddock.net/apple-touch-icon.png">
<link rel="alternate" hreflang="en" href="http://davidcraddock.net/2011/09/07/jsoup-method-for-page-scraping/">

<meta name="twitter:title" content="JSoup Method for Page Scraping | DavidCraddock.net" />
<meta name="twitter:description" content="I&rsquo;m currently in the process of writing a web scraper for the forums on Gaia Online. Previously, I used to use Python to develop web scrapers, with the very handy Python library BeautifulSoup. Java has an equivalent called JSoup.
Here I have written a class which is extended by each class in my project that wants to scrape HTML. This &lsquo;Scraper&rsquo; class deals with the fetching of the HTML and converting it into a JSoup tree to be navigated and have the data picked out of." />
<meta property="og:title" content="JSoup Method for Page Scraping | DavidCraddock.net" />
<meta property="og:description" content="I&rsquo;m currently in the process of writing a web scraper for the forums on Gaia Online. Previously, I used to use Python to develop web scrapers, with the very handy Python library BeautifulSoup. Java has an equivalent called JSoup.
Here I have written a class which is extended by each class in my project that wants to scrape HTML. This &lsquo;Scraper&rsquo; class deals with the fetching of the HTML and converting it into a JSoup tree to be navigated and have the data picked out of." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://davidcraddock.net/2011/09/07/jsoup-method-for-page-scraping/" />
<meta property="article:section" content="posts" />
  <meta property="article:published_time" content="2011-09-07T18:35:17&#43;00:00" />
  <meta property="article:modified_time" content="2011-09-07T18:35:17&#43;00:00" />


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://davidcraddock.net/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "JSoup Method for Page Scraping",
      "item": "http://davidcraddock.net/2011/09/07/jsoup-method-for-page-scraping/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "JSoup Method for Page Scraping | DavidCraddock.net",
  "name": "JSoup Method for Page Scraping",
  "description": "I\u0026rsquo;m currently in the process of writing a web scraper for the forums on Gaia Online. Previously, I used to use Python to develop web scrapers, with the very handy Python library BeautifulSoup. Java has an equivalent called JSoup.\nHere I have written a class which is extended by each class in my project that wants to scrape HTML. This \u0026lsquo;Scraper\u0026rsquo; class deals with the fetching of the HTML and converting it into a JSoup tree to be navigated and have the data picked out of.",
  "keywords": [
    
  ],
  "wordCount" : "485",
  "inLanguage": "en",
  "datePublished": "2011-09-07T18:35:17Z",
  "dateModified": "2011-09-07T18:35:17Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://davidcraddock.net/2011/09/07/jsoup-method-for-page-scraping/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "DavidCraddock.net",
    "logo": {
      "@type": "ImageObject",
      "url": "http://davidcraddock.net/favicon.ico"
    }
  }
}
</script>
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary-bg: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list-page {
                background: var(--theme);
            }

            .list-page:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list-page:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

</head>

<body class=" type-posts kind-page layout-" id="top"><script data-no-instant>
function switchTheme(theme) {
  switch (theme) {
    case 'light':
      document.body.classList.remove('dark');
      break;
    case 'dark':
      document.body.classList.add('dark');
      break;
    
    default:
      if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
      }
  }
}

function isDarkTheme() {
  return document.body.className.includes("dark");
}

function getPrefTheme() {
  return localStorage.getItem("pref-theme");
}

function setPrefTheme(theme) {
  switchTheme(theme)
  localStorage.setItem("pref-theme", theme);
}

const toggleThemeCallbacks = {}
toggleThemeCallbacks['main'] = (isDark) => {
  
  if (isDark) {
    setPrefTheme('light');
  } else {
    setPrefTheme('dark');
  }
}




window.addEventListener('toggle-theme', function() {
  
  const isDark = isDarkTheme()
  for (const key in toggleThemeCallbacks) {
    toggleThemeCallbacks[key](isDark)
  }
});


function toggleThemeListener() {
  
  window.dispatchEvent(new CustomEvent('toggle-theme'));
}

</script>
<script>
  
  (function() {
    const defaultTheme = 'auto';
    const prefTheme = getPrefTheme();
    const theme = prefTheme ? prefTheme : defaultTheme;

    switchTheme(theme);
  })();
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://davidcraddock.net/" accesskey="h" title="DavidCraddock.net (Alt + H)">DavidCraddock.net</a>
            <span class="logo-switches">
            </span>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main post">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://davidcraddock.net/">Home</a>&nbsp;Â»&nbsp;<a href="http://davidcraddock.net/posts/">Posts</a></div><h1 class="post-title">JSoup Method for Page Scraping</h1>
    <div class="post-meta"><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6" style="user-select: text;"></line><line x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10" style="user-select: text;"></line></svg>
  <span>September 7, 2011</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><circle cx="12" cy="12" r="9"></circle><polyline points="12 7 12 12 15 15"></polyline></svg>
  <span>3 min</span></span>

      
      
    </div>
  </header> 


<div>
    <ul>
        <li><a href="/">Blog Posts</a></li>
        <li><a href="/music">My Music</a></li>
        <li><a href="/my-home-network">My Home Network</a></li>
        <li><a href="/my-gaming-setup">My Gaming Setup</a></li>
        <li><a href="/my-mobile-laptop-setup">My Mobile Laptop Setup</a></li>
        <li><a href="/my-work-computer">My Work Computer</a></li>
    </ul>
</div>
  <div class="post-content"><p><a href="/wp-content/uploads/2011/09/soup.jpg"><img loading="lazy" src="/wp-content/uploads/2011/09/soup.jpg" type="" alt="Soup bowl"  /></a></p>
<p>I&rsquo;m currently in the process of writing a web scraper for the forums on <a href="http://www.gaiaonline.com/forum" title="Gaia Online">Gaia Online</a>. Previously, I used to use Python to develop web scrapers, with the very handy Python library <a href="http://www.crummy.com/software/BeautifulSoup/" title="BeautifulSoup">BeautifulSoup</a>. Java has an equivalent called JSoup.</p>
<p>Here I have written a class which is extended by each class in my project that wants to scrape HTML. This &lsquo;Scraper&rsquo; class deals with the fetching of the HTML and converting it into a JSoup tree to be navigated and have the data picked out of. It advertises itself as a &lsquo;web spider&rsquo; type of web agent and also adds a 0-7 second random wait before fetching the page to make sure it isn&rsquo;t used to overload a web server. It also converts the entire page to ASCII, which may not be the best thing to do for multi-language web pages, but certainly has made the scraping of the English language site Gaia Online much easier.</p>
<p>Here it is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>import java.io.IOException;
</span></span><span style="display:flex;"><span>import java.io.InputStream;
</span></span><span style="display:flex;"><span>import java.io.StringWriter;
</span></span><span style="display:flex;"><span>import java.text.Normalizer;
</span></span><span style="display:flex;"><span>import java.util.Random;
</span></span><span style="display:flex;"><span>import org.apache.commons.io.IOUtils;
</span></span><span style="display:flex;"><span>import org.apache.http.HttpEntity;
</span></span><span style="display:flex;"><span>import org.apache.http.HttpResponse;
</span></span><span style="display:flex;"><span>import org.apache.http.client.HttpClient;
</span></span><span style="display:flex;"><span>import org.apache.http.client.methods.HttpGet;
</span></span><span style="display:flex;"><span>import org.apache.http.impl.client.DefaultHttpClient;
</span></span><span style="display:flex;"><span>import org.jsoup.Jsoup;
</span></span><span style="display:flex;"><span>import org.jsoup.nodes.Document;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>/**
</span></span><span style="display:flex;"><span>* Generic scraper object that contains the basic methods required to fetch
</span></span><span style="display:flex;"><span>* and parse HTML content. Extended by other classes that need to scrape.
</span></span><span style="display:flex;"><span>*
</span></span><span style="display:flex;"><span>* @author David
</span></span><span style="display:flex;"><span>*/
</span></span><span style="display:flex;"><span>public class Scraper {
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        public String pageHTML = &#34;&#34;; // the HTML for the page
</span></span><span style="display:flex;"><span>        public Document pageSoup; // the JSoup scraped hierachy for the page
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        public String fetchPageHTML(String URL) throws IOException{
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            // this makes sure we don&#39;t scrape the same page twice
</span></span><span style="display:flex;"><span>            if(this.pageHTML != &#34;&#34;){
</span></span><span style="display:flex;"><span>                return this.pageHTML;
</span></span><span style="display:flex;"><span>            }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            System.getProperties().setProperty(&#34;httpclient.useragent&#34;, &#34;spider&#34;);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            Random randomGenerator = new Random();
</span></span><span style="display:flex;"><span>            int sleepTime = randomGenerator.nextInt(7000);
</span></span><span style="display:flex;"><span>            try{
</span></span><span style="display:flex;"><span>                Thread.sleep(sleepTime); //sleep for x milliseconds
</span></span><span style="display:flex;"><span>            }catch(Exception e){
</span></span><span style="display:flex;"><span>                // only fires if topic is interruped by another process, should never happen
</span></span><span style="display:flex;"><span>            }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            String pageHTML = &#34;&#34;;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            HttpClient httpclient = new DefaultHttpClient();
</span></span><span style="display:flex;"><span>            HttpGet httpget = new HttpGet(URL);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>                HttpResponse response = httpclient.execute(httpget);
</span></span><span style="display:flex;"><span>                HttpEntity entity = response.getEntity();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>                if (entity != null) {
</span></span><span style="display:flex;"><span>                    InputStream instream = entity.getContent();
</span></span><span style="display:flex;"><span>                    String encoding = &#34;UTF-8&#34;;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>                    StringWriter writer = new StringWriter();
</span></span><span style="display:flex;"><span>                    IOUtils.copy(instream, writer, encoding);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>                    pageHTML = writer.toString();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>                    // convert entire page scrape to ASCII-safe string
</span></span><span style="display:flex;"><span>                    pageHTML = Normalizer.normalize(pageHTML, Normalizer.Form.NFD).replaceAll(&#34;[^\p{ASCII}]&#34;, &#34;&#34;);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>                }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>                return pageHTML;
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        public Document fetchPageSoup(String pageHTML) throws FetchSoupException{
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            // this makes sure we don&#39;t soupify the same page twice
</span></span><span style="display:flex;"><span>            if(this.pageSoup != null){
</span></span><span style="display:flex;"><span>                return this.pageSoup;
</span></span><span style="display:flex;"><span>            }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            if(pageHTML.equalsIgnoreCase(&#34;&#34;)){
</span></span><span style="display:flex;"><span>                throw new FetchSoupException(&#34;We have no supplied HTML to soupify.&#34;);
</span></span><span style="display:flex;"><span>            }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            Document pageSoup = Jsoup.parse(pageHTML);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            return pageSoup;
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Then each class subclasses this scraper class, and adds the actual drilling down through the JSoup hierachy tree to get what is required:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>this.pageHTML = this.fetchPageHTML(this.rootURL);
</span></span><span style="display:flex;"><span>this.pageSoup = this.fetchPageSoup(this.pageHTML);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>// get the first
</span></span><span style="display:flex;"><span>.. section on the page
</span></span><span style="display:flex;"><span>Element forumPageLinkSection = this.pageSoup.getElementsByAttributeValue(&#34;id&#34;,&#34;forum_hd_topic_pagelinks&#34;).first();
</span></span><span style="display:flex;"><span>// get all the links in the above
</span></span><span style="display:flex;"><span> section
</span></span><span style="display:flex;"><span>Elements forumPageLinks = forumPageLinkSection.getElementsByAttribute(&#34;href&#34;);
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><p>I&rsquo;ve found that this method provides a simple and effective way of scraping pages and using the resultant JSoup tree to pick out important data.</p>


  </div>

  <footer class="post-footer">
  </footer>
    <div class="comments-separator"></div>
</article>
    </main>
    
<footer class="footer">
  <span>&copy; 2024 <a href="http://davidcraddock.net/">DavidCraddock.net</a></span><span style="display: inline-block; margin-left: 1em;">
    <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a>
  </span>
  <span style="display: inline-block; margin-left: 1em;">
    Powered by
    <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
    <a href="https://github.com/reorx/hugo-PaperModX/" rel="noopener" target="_blank">PaperModX</a>
  </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
    <path d="M12 6H0l6-6z" />
  </svg>
</a>

<script>
  (function() {
     
    const disableThemeToggle = '1' == '1';
    if (disableThemeToggle) {
      return;
    }

    let button = document.getElementById("theme-toggle")
    
    button.removeEventListener('click', toggleThemeListener)
    
    button.addEventListener('click', toggleThemeListener)
  })();
</script>

<script>
  (function () {
    let menu = document.getElementById('menu')
    if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
    }

    const disableSmoothScroll = '' == '1';
    const enableInstantClick = '1' == '1';
    
    if (window.matchMedia('(prefers-reduced-motion: reduce)').matches || disableSmoothScroll || enableInstantClick) {
      return;
    }
    
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
        e.preventDefault();
        var id = this.getAttribute("href").substr(1);
        document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
          behavior: "smooth"
        });
        if (id === "top") {
          history.replaceState(null, null, " ");
        } else {
          history.pushState(null, null, `#${id}`);
        }
      });
    });
  })();
</script>
<script>
  var mybutton = document.getElementById("top-link");
  window.onscroll = function () {
    if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
      mybutton.style.visibility = "visible";
      mybutton.style.opacity = "1";
    } else {
      mybutton.style.visibility = "hidden";
      mybutton.style.opacity = "0";
    }
  };
</script>
<script>
  if (window.scrollListeners) {
    
    for (const listener of scrollListeners) {
      window.removeEventListener('scroll', listener)
    }
  }
  window.scrollListeners = []
</script>



<script src="/js/medium-zoom.min.js" data-no-instant
></script>
<script>
  document.querySelectorAll('pre > code').forEach((codeblock) => {
    const container = codeblock.parentNode.parentNode;

    const copybutton = document.createElement('button');
    copybutton.classList.add('copy-code');
    copybutton.innerText = 'copy';

    function copyingDone() {
      copybutton.innerText = 'copied!';
      setTimeout(() => {
        copybutton.innerText = 'copy';
      }, 2000);
    }

    copybutton.addEventListener('click', (cb) => {
      if ('clipboard' in navigator) {
        navigator.clipboard.writeText(codeblock.textContent);
        copyingDone();
        return;
      }

      const range = document.createRange();
      range.selectNodeContents(codeblock);
      const selection = window.getSelection();
      selection.removeAllRanges();
      selection.addRange(range);
      try {
        document.execCommand('copy');
        copyingDone();
      } catch (e) { };
      selection.removeRange(range);
    });

    if (container.classList.contains("highlight")) {
      container.appendChild(copybutton);
    } else if (container.parentNode.firstChild == container) {
      
    } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
      
      codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
    } else {
      
      codeblock.parentNode.appendChild(copybutton);
    }
  });
</script>




<script>
  
  
  (function() {
    const enableTocScroll = '1' == '1'
    if (!enableTocScroll) {
      return
    }
    if (!document.querySelector('.toc')) {
      console.log('no toc found, ignore toc scroll')
      return
    }
    

    
    const scrollListeners = window.scrollListeners
    const headings = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]');
    const activeClass = 'active';

    
    let activeHeading = headings[0];
    getLinkByHeading(activeHeading).classList.add(activeClass);

    const onScroll = () => {
      const passedHeadings = [];
      for (const h of headings) {
        
        if (getOffsetTop(h) < 5) {
          passedHeadings.push(h)
        } else {
          break;
        }
      }
      if (passedHeadings.length > 0) {
        newActiveHeading = passedHeadings[passedHeadings.length - 1];
      } else {
        newActiveHeading = headings[0];
      }
      if (activeHeading != newActiveHeading) {
        getLinkByHeading(activeHeading).classList.remove(activeClass);
        activeHeading = newActiveHeading;
        getLinkByHeading(activeHeading).classList.add(activeClass);
      }
    }

    let timer = null;
    const scrollListener = () => {
      if (timer !== null) {
        clearTimeout(timer)
      }
      timer = setTimeout(onScroll, 50)
    }
    window.addEventListener('scroll', scrollListener, false);
    scrollListeners.push(scrollListener)

    function getLinkByHeading(heading) {
      const id = encodeURI(heading.getAttribute('id')).toLowerCase();
      return document.querySelector(`.toc ul li a[href="#${id}"]`);
    }

    function getOffsetTop(heading) {
      if (!heading.getClientRects().length) {
        return 0;
      }
      let rect = heading.getBoundingClientRect();
      return rect.top
    }
  })();
  </script>

<script src="/js/instantclick.min.js" data-no-instant
></script>
<script data-no-instant>
  
  
  
  
  
  
  InstantClick.init();
</script>
</body>

</html>
