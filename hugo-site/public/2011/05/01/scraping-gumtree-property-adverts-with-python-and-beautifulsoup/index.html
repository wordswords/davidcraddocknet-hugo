<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Scraping Gumtree Property Adverts with Python and BeautifulSoup | DavidCraddock.net</title>
<meta name="keywords" content="">
<meta name="description" content="I am moving to Manchester soon, and so I thought I&rsquo;d get an idea of the housing market there by scraping all the Manchester Gumtree property adverts into a MySQL database. Once in the database, I could do things like find the average monthly price for a 2 bedroom flat in an area, and spot bargains through using standard deviation from the mean on the price through using simple SQL queries via phpMyAdmin.">
<meta name="author" content="David Craddock">
<link rel="canonical" href="https://davidcraddock.net/2011/05/01/scraping-gumtree-property-adverts-with-python-and-beautifulsoup/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.41b9674e7dd0324dbc4c2ab940c0957033436cc108e55f3d6ceed518637f37d6.css" integrity="sha256-QblnTn3QMk28TCq5QMCVcDNDbMEI5V89bO7VGGN/N9Y=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://davidcraddock.net/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://davidcraddock.net/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://davidcraddock.net/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://davidcraddock.net/apple-touch-icon.png">
<link rel="mask-icon" href="https://davidcraddock.net/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://davidcraddock.net/2011/05/01/scraping-gumtree-property-adverts-with-python-and-beautifulsoup/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Lexend&display=swap" rel="stylesheet">
<meta property="og:title" content="Scraping Gumtree Property Adverts with Python and BeautifulSoup" />
<meta property="og:description" content="I am moving to Manchester soon, and so I thought I&rsquo;d get an idea of the housing market there by scraping all the Manchester Gumtree property adverts into a MySQL database. Once in the database, I could do things like find the average monthly price for a 2 bedroom flat in an area, and spot bargains through using standard deviation from the mean on the price through using simple SQL queries via phpMyAdmin." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://davidcraddock.net/2011/05/01/scraping-gumtree-property-adverts-with-python-and-beautifulsoup/" />
<meta property="og:image" content="https://davidcraddock.net/mugshot.png" />
<meta property="article:section" content="posts" />
<meta property="article:published_time" content="2011-05-01T14:07:02+00:00" />
<meta property="article:modified_time" content="2011-05-01T14:07:02+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://davidcraddock.net/mugshot.png" />
<meta name="twitter:title" content="Scraping Gumtree Property Adverts with Python and BeautifulSoup"/>
<meta name="twitter:description" content="I am moving to Manchester soon, and so I thought I&rsquo;d get an idea of the housing market there by scraping all the Manchester Gumtree property adverts into a MySQL database. Once in the database, I could do things like find the average monthly price for a 2 bedroom flat in an area, and spot bargains through using standard deviation from the mean on the price through using simple SQL queries via phpMyAdmin."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://davidcraddock.net/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Scraping Gumtree Property Adverts with Python and BeautifulSoup",
      "item": "https://davidcraddock.net/2011/05/01/scraping-gumtree-property-adverts-with-python-and-beautifulsoup/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Scraping Gumtree Property Adverts with Python and BeautifulSoup",
  "name": "Scraping Gumtree Property Adverts with Python and BeautifulSoup",
  "description": "I am moving to Manchester soon, and so I thought I\u0026rsquo;d get an idea of the housing market there by scraping all the Manchester Gumtree property adverts into a MySQL database. Once in the database, I could do things like find the average monthly price for a 2 bedroom flat in an area, and spot bargains through using standard deviation from the mean on the price through using simple SQL queries via phpMyAdmin.\n",
  "keywords": [
    
  ],
  "articleBody": "I am moving to Manchester soon, and so I thought I’d get an idea of the housing market there by scraping all the Manchester Gumtree property adverts into a MySQL database. Once in the database, I could do things like find the average monthly price for a 2 bedroom flat in an area, and spot bargains through using standard deviation from the mean on the price through using simple SQL queries via phpMyAdmin.\nI really like the Python library BeautifulSoup for writing scrapers, there is also a Java version called JSoup. BeautifulSoup does a really good job of tolerating markup mistakes in the input data, and transforms a page into a tree structure that is easy to work with.\nI chose the following layout for the program:\nadvert.py - Stores all information about each property advert, with a ‘save’ method that inserts the data into the mysql database listing.py - Stores all the information on each listing page, which is broken down into links for specific adverts, and also the link to the next listing page in the sequence (ie: the ’next page’ link) scrapeAdvert.py - When given an advert URL, this creates and populates an advert object scrapeListing.py - When given a listing URL, this creates and populates a listing object scrapeSequence.py - This walks through a series of listings, calling scrapeListing and scrapeAdvert for all of them, and finishes when there are no more listings in the sequence to scrape\nHere is the MySQL table I created for this project (which you will have to setup if you want to run the scraper):\n-- -- Database: `manchester` -- -- -------------------------------------------------------- -- -- Table structure for table `adverts` -- CREATE TABLE IF NOT EXISTS `adverts` ( `url` varchar(255) NOT NULL, `title` text NOT NULL, `pricePW` int(10) unsigned NOT NULL, `pricePCM` int(11) NOT NULL, `location` text NOT NULL, `dateAvailable` date NOT NULL, `propertyType` text NOT NULL, `bedroomNumber` int(11) NOT NULL, `description` text NOT NULL, PRIMARY KEY (`url`) ) ENGINE=MyISAM DEFAULT CHARSET=latin1; PricePCM is price per calendar month, PricePW is price per week. Usually each advert with have one or the other specified.\nadvert.py:\nimport MySQLdb import chardet import sys class advert: url = \"\" title = \"\" pricePW = 0 pricePCM = 0 location = \"\" dateAvailable = \"\" propertyType = \"\" bedroomNumber = 0 description = \"\" def save(self): # you will need to change the following to match your mysql credentials: db=MySQLdb.connect(\"localhost\",\"root\",\"secret\",\"manchester\") c=db.cursor() self.description = unicode(self.description, errors='replace') self.description = self.description.encode('ascii','ignore') # TODO: might need to convert the other strings in the advert if there are any unicode conversetion errors sql = \"INSERT INTO adverts (url,title,pricePCM,pricePW,location,dateAvailable,propertyType,bedroomNumber,description) VALUES('\"+self.url+\"','\"+self.title+\"',\"+str(self.pricePCM)+\",\"+str(self.pricePW)+\",'\"+self.location+\"','\"+self.dateAvailable+\"','\"+self.propertyType+\"',\"+str(self.bedroomNumber)+\",'\"+self.description+\"' )\" c.execute(sql) In advert.py we convert the unicode output that BeautifulSoup gives us into plain ASCII so that we can put it in the MySQL database without any problems. I could have used Unicode in the database as well, but the chances of really needing Unicode for representing Gumtree ads is quite slim. If you intend to use this code then you will also want to enter the MySQL credentials for your database.\nlisting.py:\nclass listing: url=\"\" adverturls=[] nextLink=\"\" def addAdvertURL(self,url): self.adverturls.append(url) scrapeAdvert.py:\nfrom BeautifulSoup import BeautifulSoup # For processing HTML import urllib2 from advert import advert import time class scrapeAdvert: page = \"\" soup = \"\" def scrape(self,advertURL): # give it a bit of time so gumtree doesn't # ban us time.sleep(2) url = advertURL # print \"-- scraping \"+url+\" --\" page = urllib2.urlopen(url) self.soup = BeautifulSoup(page) self.anAd = advert() self.anAd.url = url self.anAd.title = self.extractTitle() self.anAd.pricePW = self.extractPricePW() self.anAd.pricePCM = self.extractPricePCM() self.anAd.location = self.extractLocation() self.anAd.dateAvailable = self.extractDateAvailable() self.anAd.propertyType = self.extractPropertyType() self.anAd.bedroomNumber = self.extractBedroomNumber() self.anAd.description = self.extractDescription() def extractTitle(self): location = self.soup.find('h1') string = location.contents[0] stripped = ' '.join(string.split()) stripped = stripped.replace(\"'\",'\"') # print '|' + stripped + '|' return stripped def extractPricePCM(self): location = self.soup.find('span',attrs={\"class\" : \"price\"}) try: string = location.contents[0] string.index('pcm') except AttributeError: # for ads with no prices set return 0 except ValueError: # for ads with pw specified return 0 stripped = string.replace('£','') stripped = stripped.replace('pcm','') stripped = stripped.replace(',','') stripped = stripped.replace(\"'\",'\"') stripped = ' '.join(stripped.split()) # print '|' + stripped + '|' return int(stripped) def extractPricePW(self): location = self.soup.find('span',attrs={\"class\" : \"price\"}) try: string = location.contents[0] string.index('pw') except AttributeError: # for ads with no prices set return 0 except ValueError: # for ads with pcm specified return 0 stripped = string.replace('£','') stripped = stripped.replace('pw','') stripped = stripped.replace(',','') stripped = stripped.replace(\"'\",'\"') stripped = ' '.join(stripped.split()) # print '|' + stripped + '|' return int(stripped) def extractLocation(self): location = self.soup.find('span',attrs={\"class\" : \"location\"}) string = location.contents[0] stripped = ' '.join(string.split()) stripped = stripped.replace(\"'\",'\"') # print '|' + stripped + '|' return stripped def extractDateAvailable(self): current_year = '2011' ul = self.soup.find('ul',attrs={\"id\" : \"ad-details\"}) firstP = ul.findAll('p')[0] string = firstP.contents[0] stripped = ' '.join(string.split()) date_to_convert = stripped + '/'+current_year try: date_object = time.strptime(date_to_convert, \"%d/%m/%Y\") except ValueError: # for adverts with no date available return \"\" full_date = time.strftime('%Y-%m-%d %H:%M:%S', date_object) # print '|' + full_date + '|' return full_date def extractPropertyType(self): ul = self.soup.find('ul',attrs={\"id\" : \"ad-details\"}) try: secondP = ul.findAll('p')[1] except IndexError: # for properties with no type return \"\" string = secondP.contents[0] stripped = ' '.join(string.split()) stripped = stripped.replace(\"'\",'\"') # print '|' + stripped + '|' return stripped def extractBedroomNumber(self): ul = self.soup.find('ul',attrs={\"id\" : \"ad-details\"}) try: thirdP = ul.findAll('p')[2] except IndexError: # for properties with no bedroom number return 0 string = thirdP.contents[0] stripped = ' '.join(string.split()) stripped = stripped.replace(\"'\",'\"') # print '|' + stripped + '|' return stripped def extractDescription(self): div = self.soup.find('div',attrs={\"id\" : \"description\"}) description = div.find('p') contents = description.renderContents() contents = contents.replace(\"'\",'\"') # print '|' + contents + '|' return contents In scrapeAdvert.py there are a lot of string manipulation statements to pull out any unwanted characters, such as the ‘pw’ characters (short for per week) found in the price string, which we need to remove in order to store the property price per week as an integer.\nUsing BeautifulSoup to pull out elements is quite easy, for example:\nul = self.soup.find('ul',attrs={\"id\" : \"ad-details\"}) That finds all the HTML elements under the tag id=“ad-details”, so all the list elements in that list. More detail can be found in the Beautiful Soup documentation which is very good.\nscrapeListing.py:\nfrom BeautifulSoup import BeautifulSoup # For processing HTML import urllib2 from listing import listing import time class scrapeListing: soup = \"\" url = \"\" aListing = \"\" def scrape(self,url): # give it a bit of time so gumtree doesn't # ban us time.sleep(3) print \"scraping url = \"+str(url) page = urllib2.urlopen(url) self.soup = BeautifulSoup(page) self.aListing = listing() self.aListing.url = url self.aListing.adverturls = self.extractAdvertURLs() self.aListing.nextLink = self.extractNextLink() def extractAdvertURLs(self): toReturn = [] h3s = self.soup.findAll(\"h3\") for h3 in h3s: links = h3.findAll('a',{\"class\":\"summary\"}) for link in links: print \"|\"+link['href']+\"|\" toReturn.append(link['href']) return toReturn def extractNextLink(self): links = self.soup.findAll(\"a\",{\"class\":\"next\"}) try: print \"\u003e\"+links[0]['href']+\"\u003e\" except IndexError: # if there is no 'next' link found.. return \"\" return links[0]['href'] The extractNextLink method here extracts the pagination ’next’ link which will bring up the next listing page from the selection of listing pages to browse. We use it to step through the pagination ‘sequence’ of resultant listing pages.\nscrapeSequence.py:\nfrom scrapeListing import scrapeListing from scrapeAdvert import scrapeAdvert from listing import listing from advert import advert import MySQLdb import _mysql_exceptions # change this to the gumtree page you want to start scraping from url = \"http://www.gumtree.com/flats-and-houses-for-rent/salford-quays\" while url != None: print \"scraping URL = \"+url sl = \"\" sl = scrapeListing() sl.scrape(url) for advertURL in sl.aListing.adverturls: sa = \"\" sa = scrapeAdvert() sa.scrape(advertURL) try: sa.anAd.save() except _mysql_exceptions.IntegrityError: print \"** Advert \" + sa.anAd.url + \" already saved **\" sa.onAd = \"\" url = \"\" if sl.aListing.nextLink: print \"nextLink = \"+sl.aListing.nextLink url = sl.aListing.nextLink else: print 'all done.' break This is the file you run to kick off the scrape. It uses an MySQL IntegrityError try/except block to pick out when an advert has already been entered into the database, this will throw an error because the URL of the advert is the primary key in the database. So no two records can have the same primary key.\nThe URL you provide it above gives you the starting page from which to scrape from.\nThe above code worked well for scraping several hundred Manchester Gumtree ads into a database, from which point I was able to use a combination of phpMyAdmin and OpenOffice Spreadsheet to analyse the data and find out useful statistics about the property market in said area.\nDownload the scraper source code in a tar.gz archive\nNote: Due to the nature of web scraping, if - or more accurately, when - Gumtree changes its user interface, the scraper I have written will need to be tweaked accordingly to find the right data. This is meant to be an informative tutorial, not a finished product.\n",
  "wordCount" : "1456",
  "inLanguage": "en",
  "image": "https://davidcraddock.net/mugshot.png","datePublished": "2011-05-01T14:07:02Z",
  "dateModified": "2011-05-01T14:07:02Z",
  "author":{
    "@type": "Person",
    "name": "David Craddock"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://davidcraddock.net/2011/05/01/scraping-gumtree-property-adverts-with-python-and-beautifulsoup/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "DavidCraddock.net",
    "logo": {
      "@type": "ImageObject",
      "url": "https://davidcraddock.net/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://davidcraddock.net/" accesskey="h" title="DavidCraddock.net (Alt + H)">DavidCraddock.net</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://davidcraddock.net/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://davidcraddock.net/my-bike/" title="Electric Mountain Bike">
                    <span>Electric Mountain Bike</span>
                </a>
            </li>
            <li>
                <a href="https://davidcraddock.net/my-gaming-setup/" title="Gaming Setup">
                    <span>Gaming Setup</span>
                </a>
            </li>
            <li>
                <a href="https://davidcraddock.net/my-home-network/" title="HomeLab Network">
                    <span>HomeLab Network</span>
                </a>
            </li>
            <li>
                <a href="https://davidcraddock.net/my-mobile-laptop-setup/" title="Mobile Working Setup">
                    <span>Mobile Working Setup</span>
                </a>
            </li>
            <li>
                <a href="https://davidcraddock.net/security-research/" title="Security Research">
                    <span>Security Research</span>
                </a>
            </li>
            <li>
                <a href="https://davidcraddock.net/my-work-computer/" title="WFH Setup">
                    <span>WFH Setup</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://davidcraddock.net/">Home</a>&nbsp;»&nbsp;<a href="https://davidcraddock.net/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Scraping Gumtree Property Adverts with Python and BeautifulSoup
    </h1>
    <div class="post-meta"><span title='2011-05-01 14:07:02 +0000 +0000'>May 1, 2011</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;David Craddock

</div>
  </header> 


  <div class="post-content"><p>I am moving to Manchester soon, and so I thought I&rsquo;d get an idea of the housing market there by scraping all the Manchester Gumtree property adverts into a MySQL database. Once in the database, I could do things like find the average monthly price for a 2 bedroom flat in an area, and spot bargains through using standard deviation from the mean on the price through using simple SQL queries via <a href="http://www.phpmyadmin.net/home_page/index.php">phpMyAdmin</a>.</p>
<p>I really like the Python library <a href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> for writing scrapers, there is also a Java version called <a href="http://jsoup.org/">JSoup</a>. BeautifulSoup does a really good job of tolerating markup mistakes in the input data, and transforms a page into a tree structure that is easy to work with.</p>
<p>I chose the following layout for the program:</p>
<p><strong>advert.py</strong> - Stores all information about each property advert, with a &lsquo;save&rsquo; method that inserts the data into the mysql database
<strong>listing.py</strong> - Stores all the information on each listing page, which is broken down into links for specific adverts, and also the link to the next listing page in the sequence (ie: the &rsquo;next page&rsquo; link)
<strong>scrapeAdvert.py</strong> - When given an advert URL, this creates and populates an advert object
<strong>scrapeListing.py</strong> - When given a listing URL, this creates and populates a listing object
<strong>scrapeSequence.py</strong> - This walks through a series of listings, calling scrapeListing and scrapeAdvert for all of them, and finishes when there are no more listings in the sequence to scrape</p>
<p>Here is the MySQL table I created for this project (which you will have to setup if you want to run the scraper):</p>
<pre tabindex="0"><code>--
-- Database: `manchester`
--

-- --------------------------------------------------------

--
-- Table structure for table `adverts`
--

CREATE TABLE IF NOT EXISTS `adverts` (
  `url` varchar(255) NOT NULL,
  `title` text NOT NULL,
  `pricePW` int(10) unsigned NOT NULL,
  `pricePCM` int(11) NOT NULL,
  `location` text NOT NULL,
  `dateAvailable` date NOT NULL,
  `propertyType` text NOT NULL,
  `bedroomNumber` int(11) NOT NULL,
  `description` text NOT NULL,
  PRIMARY KEY (`url`)
) ENGINE=MyISAM DEFAULT CHARSET=latin1;
</code></pre><p>PricePCM is price per calendar month, PricePW is price per week. Usually each advert with have one or the other specified.</p>
<p><strong>advert.py:</strong></p>
<pre tabindex="0"><code>import MySQLdb
import chardet
import sys

class advert:

        url = &#34;&#34;
        title = &#34;&#34;
        pricePW = 0
        pricePCM = 0
        location = &#34;&#34;
        dateAvailable = &#34;&#34;
        propertyType = &#34;&#34;
        bedroomNumber = 0
        description = &#34;&#34;

        def save(self):
                # you will need to change the following to match your mysql credentials:
                db=MySQLdb.connect(&#34;localhost&#34;,&#34;root&#34;,&#34;secret&#34;,&#34;manchester&#34;)
                c=db.cursor()

                self.description = unicode(self.description, errors=&#39;replace&#39;)
                self.description = self.description.encode(&#39;ascii&#39;,&#39;ignore&#39;)
                # TODO: might need to convert the other strings in the advert if there are any unicode conversetion errors

                sql = &#34;INSERT INTO adverts (url,title,pricePCM,pricePW,location,dateAvailable,propertyType,bedroomNumber,description) VALUES(&#39;&#34;+self.url+&#34;&#39;,&#39;&#34;+self.title+&#34;&#39;,&#34;+str(self.pricePCM)+&#34;,&#34;+str(self.pricePW)+&#34;,&#39;&#34;+self.location+&#34;&#39;,&#39;&#34;+self.dateAvailable+&#34;&#39;,&#39;&#34;+self.propertyType+&#34;&#39;,&#34;+str(self.bedroomNumber)+&#34;,&#39;&#34;+self.description+&#34;&#39; )&#34;

                c.execute(sql)
</code></pre><p>In advert.py we convert the unicode output that BeautifulSoup gives us into plain ASCII so that we can put it in the MySQL database without any problems. I could have used Unicode in the database as well, but the chances of really needing Unicode for representing Gumtree ads is quite slim. If you intend to use this code then you will also want to enter the MySQL credentials for your database.</p>
<p><strong>listing.py:</strong></p>
<pre tabindex="0"><code>class listing:

        url=&#34;&#34;
        adverturls=[]
        nextLink=&#34;&#34;

        def addAdvertURL(self,url):

                self.adverturls.append(url)
</code></pre><p><strong>scrapeAdvert.py:</strong></p>
<pre tabindex="0"><code>from BeautifulSoup import BeautifulSoup          # For processing HTML
import urllib2
from advert import advert
import time

class scrapeAdvert:

        page = &#34;&#34;
        soup = &#34;&#34;

        def scrape(self,advertURL):

                # give it a bit of time so gumtree doesn&#39;t
                # ban us
                time.sleep(2)

                url = advertURL
                # print &#34;-- scraping &#34;+url+&#34; --&#34;
                page = urllib2.urlopen(url)
                self.soup = BeautifulSoup(page)

                self.anAd = advert()

                self.anAd.url = url
                self.anAd.title = self.extractTitle()
                self.anAd.pricePW = self.extractPricePW()
                self.anAd.pricePCM = self.extractPricePCM()

                self.anAd.location = self.extractLocation()
                self.anAd.dateAvailable = self.extractDateAvailable()
                self.anAd.propertyType = self.extractPropertyType()
                self.anAd.bedroomNumber = self.extractBedroomNumber()
                self.anAd.description = self.extractDescription()

        def extractTitle(self):

                location = self.soup.find(&#39;h1&#39;)
                string = location.contents[0]
                stripped = &#39; &#39;.join(string.split())
                stripped = stripped.replace(&#34;&#39;&#34;,&#39;&#34;&#39;)
                # print &#39;|&#39; + stripped + &#39;|&#39;
                return stripped

        def extractPricePCM(self):

                location = self.soup.find(&#39;span&#39;,attrs={&#34;class&#34; : &#34;price&#34;})
                try:
                        string = location.contents[0]
                        string.index(&#39;pcm&#39;)
                except AttributeError: # for ads with no prices set
                        return 0
                except ValueError: # for ads with pw specified
                        return 0

                stripped = string.replace(&#39;£&#39;,&#39;&#39;)
                stripped = stripped.replace(&#39;pcm&#39;,&#39;&#39;)
                stripped = stripped.replace(&#39;,&#39;,&#39;&#39;)
                stripped = stripped.replace(&#34;&#39;&#34;,&#39;&#34;&#39;)
                stripped = &#39; &#39;.join(stripped.split())
                # print &#39;|&#39; + stripped + &#39;|&#39;
                return int(stripped)

        def extractPricePW(self):

                location = self.soup.find(&#39;span&#39;,attrs={&#34;class&#34; : &#34;price&#34;})
                try:
                        string = location.contents[0]
                        string.index(&#39;pw&#39;)
                except AttributeError: # for ads with no prices set
                        return 0
                except ValueError: # for ads with pcm specified
                        return 0
                stripped = string.replace(&#39;£&#39;,&#39;&#39;)
                stripped = stripped.replace(&#39;pw&#39;,&#39;&#39;)
                stripped = stripped.replace(&#39;,&#39;,&#39;&#39;)
                stripped = stripped.replace(&#34;&#39;&#34;,&#39;&#34;&#39;)
                stripped = &#39; &#39;.join(stripped.split())
                # print &#39;|&#39; + stripped + &#39;|&#39;
                return int(stripped)

        def extractLocation(self):

                location = self.soup.find(&#39;span&#39;,attrs={&#34;class&#34; : &#34;location&#34;})
                string = location.contents[0]
                stripped = &#39; &#39;.join(string.split())
                stripped = stripped.replace(&#34;&#39;&#34;,&#39;&#34;&#39;)
                # print &#39;|&#39; + stripped + &#39;|&#39;
                return stripped

        def extractDateAvailable(self):

                current_year = &#39;2011&#39;

                ul = self.soup.find(&#39;ul&#39;,attrs={&#34;id&#34; : &#34;ad-details&#34;})
                firstP = ul.findAll(&#39;p&#39;)[0]
                string = firstP.contents[0]
                stripped = &#39; &#39;.join(string.split())
                date_to_convert = stripped + &#39;/&#39;+current_year
                try:
                        date_object = time.strptime(date_to_convert, &#34;%d/%m/%Y&#34;)
                except ValueError: # for adverts with no date available
                        return &#34;&#34;

                full_date = time.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;, date_object)
                # print &#39;|&#39; + full_date + &#39;|&#39;
                return full_date

        def extractPropertyType(self):

                ul = self.soup.find(&#39;ul&#39;,attrs={&#34;id&#34; : &#34;ad-details&#34;})
                try:
                        secondP = ul.findAll(&#39;p&#39;)[1]
                except IndexError: # for properties with no type
                        return &#34;&#34;
                string = secondP.contents[0]
                stripped = &#39; &#39;.join(string.split())
                stripped = stripped.replace(&#34;&#39;&#34;,&#39;&#34;&#39;)
                # print &#39;|&#39; + stripped + &#39;|&#39;
                return stripped

        def extractBedroomNumber(self):

                ul = self.soup.find(&#39;ul&#39;,attrs={&#34;id&#34; : &#34;ad-details&#34;})
                try:
                        thirdP = ul.findAll(&#39;p&#39;)[2]
                except IndexError: # for properties with no bedroom number
                        return 0
                string = thirdP.contents[0]
                stripped = &#39; &#39;.join(string.split())
                stripped = stripped.replace(&#34;&#39;&#34;,&#39;&#34;&#39;)
                # print &#39;|&#39; + stripped + &#39;|&#39;
                return stripped

        def extractDescription(self):

                div = self.soup.find(&#39;div&#39;,attrs={&#34;id&#34; : &#34;description&#34;})
                description = div.find(&#39;p&#39;)
                contents = description.renderContents()
                contents = contents.replace(&#34;&#39;&#34;,&#39;&#34;&#39;)
                # print &#39;|&#39; + contents + &#39;|&#39;
                return contents
</code></pre><p>In scrapeAdvert.py there are a lot of string manipulation statements to pull out any unwanted characters, such as the &lsquo;pw&rsquo; characters (short for per week) found in the price string, which we need to remove in order to store the property price per week as an integer.</p>
<p>Using BeautifulSoup to pull out elements is quite easy, for example:</p>
<pre tabindex="0"><code>ul = self.soup.find(&#39;ul&#39;,attrs={&#34;id&#34; : &#34;ad-details&#34;})
</code></pre><p>That finds all the HTML elements under the tag id=&ldquo;ad-details&rdquo;, so all the list elements in that list. More detail can be found in the <a href="http://www.crummy.com/software/BeautifulSoup/documentation.html">Beautiful Soup documentation</a> which is very good.</p>
<p><strong>scrapeListing.py:</strong></p>
<pre tabindex="0"><code>from BeautifulSoup import BeautifulSoup          # For processing HTML
import urllib2
from listing import listing
import time

class scrapeListing:

        soup = &#34;&#34;
        url = &#34;&#34;
        aListing = &#34;&#34;

        def scrape(self,url):
                # give it a bit of time so gumtree doesn&#39;t
                # ban us
                time.sleep(3)

                print &#34;scraping url = &#34;+str(url)

                page = urllib2.urlopen(url)
                self.soup = BeautifulSoup(page)

                self.aListing = listing()
                self.aListing.url = url
                self.aListing.adverturls = self.extractAdvertURLs()
                self.aListing.nextLink = self.extractNextLink()

        def extractAdvertURLs(self):

                toReturn = []
                h3s = self.soup.findAll(&#34;h3&#34;)
                for h3 in h3s:
                        links = h3.findAll(&#39;a&#39;,{&#34;class&#34;:&#34;summary&#34;})
                        for link in links:
                                print &#34;|&#34;+link[&#39;href&#39;]+&#34;|&#34;
                                toReturn.append(link[&#39;href&#39;])

                return toReturn

        def extractNextLink(self):

                links = self.soup.findAll(&#34;a&#34;,{&#34;class&#34;:&#34;next&#34;})
                try:
                        print &#34;&gt;&#34;+links[0][&#39;href&#39;]+&#34;&gt;&#34;
                except IndexError: # if there is no &#39;next&#39; link found..
                        return &#34;&#34;
                return links[0][&#39;href&#39;]
</code></pre><p>The extractNextLink method here extracts the pagination &rsquo;next&rsquo; link which will bring up the next listing page from the selection of listing pages to browse. We use it to step through the pagination &lsquo;sequence&rsquo; of resultant listing pages.</p>
<p><strong>scrapeSequence.py:</strong></p>
<pre tabindex="0"><code>from scrapeListing import scrapeListing
from scrapeAdvert import scrapeAdvert
from listing import listing
from advert import advert
import MySQLdb
import _mysql_exceptions

# change this to the gumtree page you want to start scraping from
url = &#34;http://www.gumtree.com/flats-and-houses-for-rent/salford-quays&#34;

while url != None:
        print &#34;scraping URL = &#34;+url
        sl = &#34;&#34;
        sl = scrapeListing()
        sl.scrape(url)
        for advertURL in sl.aListing.adverturls:
                sa = &#34;&#34;
                sa = scrapeAdvert()
                sa.scrape(advertURL)
                try:
                        sa.anAd.save()
                except _mysql_exceptions.IntegrityError:
                        print &#34;** Advert &#34; + sa.anAd.url + &#34; already saved **&#34;
                sa.onAd = &#34;&#34;

        url = &#34;&#34;
        if sl.aListing.nextLink:
                print &#34;nextLink = &#34;+sl.aListing.nextLink
                url = sl.aListing.nextLink
        else:
                print &#39;all done.&#39;
                break
</code></pre><p>This is the file you run to kick off the scrape. It uses an MySQL IntegrityError try/except block to pick out when an advert has already been entered into the database, this will throw an error because the URL of the advert is the primary key in the database. So no two records can have the same primary key.</p>
<p>The URL you provide it above gives you the starting page from which to scrape from.</p>
<p>The above code worked well for scraping several hundred Manchester Gumtree ads into a database, from which point I was able to use a combination of phpMyAdmin and OpenOffice Spreadsheet to analyse the data and find out useful statistics about the property market in said area.</p>
<p><a href="http://www.davidcraddock.net/uploads/gumtree-scraper.tgz">Download the scraper source code in a tar.gz archive</a></p>
<p>Note: Due to the nature of web scraping, if - or more accurately, when - Gumtree changes its user interface, the scraper I have written will need to be tweaked accordingly to find the right data. This is meant to be an informative tutorial, not a finished product.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://davidcraddock.net/2011/05/04/directory-names-not-visable-under-ls-change-your-colours/">
    <span class="title">« Prev</span>
    <br>
    <span>Directory names not visable under ls? Change your colours.</span>
  </a>
  <a class="next" href="https://davidcraddock.net/2011/03/02/restful-web-services/">
    <span class="title">Next »</span>
    <br>
    <span>RESTful Web Services</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Scraping Gumtree Property Adverts with Python and BeautifulSoup on x"
            href="https://x.com/intent/tweet/?text=Scraping%20Gumtree%20Property%20Adverts%20with%20Python%20and%20BeautifulSoup&amp;url=https%3a%2f%2fdavidcraddock.net%2f2011%2f05%2f01%2fscraping-gumtree-property-adverts-with-python-and-beautifulsoup%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Scraping Gumtree Property Adverts with Python and BeautifulSoup on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdavidcraddock.net%2f2011%2f05%2f01%2fscraping-gumtree-property-adverts-with-python-and-beautifulsoup%2f&amp;title=Scraping%20Gumtree%20Property%20Adverts%20with%20Python%20and%20BeautifulSoup&amp;summary=Scraping%20Gumtree%20Property%20Adverts%20with%20Python%20and%20BeautifulSoup&amp;source=https%3a%2f%2fdavidcraddock.net%2f2011%2f05%2f01%2fscraping-gumtree-property-adverts-with-python-and-beautifulsoup%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Scraping Gumtree Property Adverts with Python and BeautifulSoup on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fdavidcraddock.net%2f2011%2f05%2f01%2fscraping-gumtree-property-adverts-with-python-and-beautifulsoup%2f&title=Scraping%20Gumtree%20Property%20Adverts%20with%20Python%20and%20BeautifulSoup">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Scraping Gumtree Property Adverts with Python and BeautifulSoup on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdavidcraddock.net%2f2011%2f05%2f01%2fscraping-gumtree-property-adverts-with-python-and-beautifulsoup%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Scraping Gumtree Property Adverts with Python and BeautifulSoup on whatsapp"
            href="https://api.whatsapp.com/send?text=Scraping%20Gumtree%20Property%20Adverts%20with%20Python%20and%20BeautifulSoup%20-%20https%3a%2f%2fdavidcraddock.net%2f2011%2f05%2f01%2fscraping-gumtree-property-adverts-with-python-and-beautifulsoup%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Scraping Gumtree Property Adverts with Python and BeautifulSoup on telegram"
            href="https://telegram.me/share/url?text=Scraping%20Gumtree%20Property%20Adverts%20with%20Python%20and%20BeautifulSoup&amp;url=https%3a%2f%2fdavidcraddock.net%2f2011%2f05%2f01%2fscraping-gumtree-property-adverts-with-python-and-beautifulsoup%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Scraping Gumtree Property Adverts with Python and BeautifulSoup on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Scraping%20Gumtree%20Property%20Adverts%20with%20Python%20and%20BeautifulSoup&u=https%3a%2f%2fdavidcraddock.net%2f2011%2f05%2f01%2fscraping-gumtree-property-adverts-with-python-and-beautifulsoup%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>© David Paul Craddock</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
